{
  "best": {
    "name": "logreg",
    "metrics": {
      "accuracy": 0.6136363636363636,
      "precision": 0.38461538461538464,
      "recall": 0.35714285714285715,
      "f1": 0.37037037037037035
    },
    "params": {
      "C": 10,
      "penalty": "l1",
      "solver": "liblinear"
    }
  },
  "all": {
    "logreg": {
      "metrics": {
        "accuracy": 0.6136363636363636,
        "precision": 0.38461538461538464,
        "recall": 0.35714285714285715,
        "f1": 0.37037037037037035
      },
      "params": {
        "C": 10,
        "penalty": "l1",
        "solver": "liblinear"
      }
    },
    "rf": {
      "metrics": {
        "accuracy": 0.6363636363636364,
        "precision": 0.375,
        "recall": 0.21428571428571427,
        "f1": 0.2727272727272727
      },
      "params": {
        "max_depth": 5,
        "min_samples_split": 5,
        "n_estimators": 50
      }
    },
    "knn": {
      "metrics": {
        "accuracy": 0.4090909090909091,
        "precision": 0.16666666666666666,
        "recall": 0.21428571428571427,
        "f1": 0.1875
      },
      "params": {
        "n_neighbors": 3,
        "weights": "distance"
      }
    },
    "svm": {
      "metrics": {
        "accuracy": 0.5681818181818182,
        "precision": 0.2727272727272727,
        "recall": 0.21428571428571427,
        "f1": 0.24
      },
      "params": {
        "kernel": "rbf",
        "C": 1.0
      }
    },
    "gb": {
      "metrics": {
        "accuracy": 0.5909090909090909,
        "precision": 0.3,
        "recall": 0.21428571428571427,
        "f1": 0.25
      },
      "params": {
        "n_estimators": 100,
        "max_depth": 3
      }
    },
    "voting": {
      "metrics": {
        "accuracy": 0.6136363636363636,
        "precision": 0.3333333333333333,
        "recall": 0.21428571428571427,
        "f1": 0.2608695652173913
      },
      "params": {
        "estimators": "logreg+rf+gb"
      }
    }
  },
  "timestamp": "2025-08-31T18:03:38.628832"
}